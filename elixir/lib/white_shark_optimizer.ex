defmodule WhiteSharkOptimizer do
  @moduledoc """
  Implements the White Shark Optimization algorithm for solving optimization problems in Nx.
  Based on
  https://www.sciencedirect.com/science/article/pii/S0950705122001897
  https://www.mathworks.com/matlabcentral/fileexchange/107365-white-shark-optimizer-wso
  """
  @derive {Nx.Container, containers: [
    :problem,
    :hyperparams,
    :key,
    :w,
    :v,
    :wgbestk,
    :w_best,
    :best_fitness,
    :fitness_results,
    :best_g_fitness
  ]}
  require Nx
  require Problem
  require Hyperparameters
  import Nx.Defn
  use SharedMacro
  def n, do: @n
  def d, do: @d

  @type t :: %WhiteSharkOptimizer{
          problem: Problem.t() | nil,
          hyperparams: Hyperparameters.t | nil,
          key: integer() | nil,
          w: Nx.Tensor.t | nil,
          v: Nx.Tensor.t | nil,
          k: integer(),
          max_iterations: integer(), #called K in the paper
          p1: Nx.Tensor.t() | nil,
          p2: Nx.Tensor.t() | nil,
          wgbestk: Nx.Tensor.t() | nil,
          best_g_fitness: float() | nil,
          w_best: Nx.Tensor.t() | nil,
          best_fitness: Nx.Tensor.t | nil,
          fitness_results: Nx.Tensor.t | nil,
          verbose: boolean(),
          fully_vectorized: boolean()
        }

  defstruct problem: nil,
            hyperparams: nil,
            key: nil,
            w: nil,
            v: nil,
            k: 0,
            max_iterations: 100,
            p1: nil,
            p2: nil,
            wgbestk: nil,
            best_g_fitness: nil,
            w_best: nil,
            best_fitness: nil,
            fitness_results: nil,
            verbose: true,
            fully_vectorized: false


  @spec compute_ps(t()) :: t()
  defp compute_ps(wso) do
    p_min = Nx.tensor(wso.hyperparams.p_min)
    p_max = Nx.tensor(wso.hyperparams.p_max)

    # Convert scalar fields to tensors with the same type as p_min.
    # This ensures that arithmetic operations are performed in the tensor realm.
    k = Nx.tensor(wso.k, type: Nx.type(p_min))
    max_iter = Nx.tensor(wso.max_iterations, type: Nx.type(p_min))

    # Compute factor = 4 * (k / max_iterations)
    factor = Nx.multiply(4, Nx.divide(k, max_iter))
    # Compute the exponential factor: exp( - (factor)^2 )
    exp_factor = Nx.exp(Nx.multiply(-1, Nx.pow(factor, 2)))

    # Compute p1 and p2 using tensor arithmetic
    diff = Nx.subtract(p_max, p_min)
    p1 = Nx.add(p_max, Nx.multiply(diff, exp_factor))
    p2 = Nx.add(p_min, Nx.multiply(diff, exp_factor))

    %{wso | p1: p1, p2: p2}
  end

  @doc """
  Initializes a new instance of the WhiteSharkOptimizer struct with the provided problem, hyperparameters, and optional configuration.

  ### Parameters:
  - `problem`: A struct defining the optimization problem to be solved. It must include the following fields:
    - `d`: Dimensionality of the problem.
    - `l`: Lower bounds for the search space as an Nx.Tensor of shape `{d}`.
    - `u`: Upper bounds for the search space as an Nx.Tensor of shape `{d}`.
    - `fun`: A fitness function of the form `(Nx.Tensor.t() -> Nx.Tensor.t())`, used to evaluate the quality of solutions matrix {n, d}.
    - The dimensionality (`d`) must match the bounds (`l` and `u`).
  - `hyperparams`: A struct specifying the algorithm's hyperparameters such as the learning rate (`mu`), and others necessary for controlling the behavior of the optimizer.
  - `opts`: A map of optional configuration values. These can include:
    - `key`: Random key for generating initial population and randomness. Defaults to a key generated by `Nx.Random.key(0)` if not provided.
    - `verbose`: If `false`, the best solution will be printed every iteration. Defaults to `true`.

  ### Raises:
  - `ArgumentError`: Raised in the following cases:
    - `problem` is `nil`, as the optimizer cannot operate without a defined problem.
    - `problem` is missing required fields (`d`, `l`, `u`, or `fun`).
    - The dimensions of the bounds (`l` and `u`) do not match the dimensionality (`d`).

  ### Returns:
  - A `WhiteSharkOptimizer` struct initialized with computed fields.
  """
  @spec new(Problem.t(), Hyperparameters.t(), map()) :: t()
  def new(%{d: _, l: _, u: _, fun: _} = problem, hyperparams, opts \\ %{}) do
    validate_problem(problem)

    key = Map.get(opts, :key, Nx.Random.key(0))

    {random_tensor, key} = Nx.Random.uniform(key, shape: {@n, @d})
    w_initial = random_tensor
          |> Nx.multiply(Nx.subtract(problem.u, problem.l))
          |> Nx.add(problem.l)
    computed_fields = %{
      w: w_initial,
      v: Nx.broadcast(0, {@n, @d}),
      key: key,
      problem: problem,
      hyperparams: hyperparams,
      w_best: w_initial,
      best_g_fitness: inf(),
      best_fitness: Nx.broadcast(inf(), {@n})
    }

    %__MODULE__{}
    |> struct(Map.merge(computed_fields, opts))
    |> compute_ps()

  end

  defp validate_problem(%{d: d, l: l, u: u}) do
    unless is_integer(d) and d > 0 do
      raise ArgumentError, "`d` must be a positive integer"
    end

    unless Nx.axis_size(l, 0) == d and Nx.axis_size(u, 0) == d do
      raise ArgumentError, "The dimensions of `l` and `u` must match `d` in the problem struct"
    end
  end

  defp validate_problem(_) do
    raise ArgumentError, "Problem struct must include fields, `l`, `u`, and `fun`"
  end

  @spec fitness_function(t()) :: t()
  defp fitness_function(wso) do
    %{wso | fitness_results: wso.problem.fun.(wso.w, wso.problem.shift)}
  end

  @spec find_wgbestk(t()) :: t()
  defp find_wgbestk(wso) do
    {new_wgbestk, new_best_g_fitness} =
      find_wgbestk(
        wso.w,
        wso.fitness_results,
        wso.best_g_fitness,
        wso.wgbestk || default_wgbestk(wso.w)
      )

    %{wso | wgbestk: new_wgbestk, best_g_fitness: new_best_g_fitness}
  end
  defp default_wgbestk(w) do
    Nx.slice(w, [0, 0], [1, Nx.axis_size(w, 1)])
  end
  defn find_wgbestk(w, fitness_results, best_g_fitness, current_wgbestk) do
    # Determine the index with the minimum fitness value.
    gbestk = Nx.argmin(fitness_results)
    # Extract that fitness value and reshape it to a scalar.
    gbestk_fitness_value =
      fitness_results
      |> Nx.slice([gbestk], [1])
      |> Nx.reshape({})

    # Use tensor comparisons (Nx.less/2) to build a condition.
    condition = Nx.less(gbestk_fitness_value, best_g_fitness)

    # Slice out the candidate best position.
    new_wgbestk = Nx.slice(w, [gbestk, 0], [1, Nx.axis_size(w, 1)])

    # Conditionally select the new values if the fitness is better,
    # otherwise keep the current ones.
    updated_wgbestk = Nx.select(condition, new_wgbestk, current_wgbestk)
    updated_best_fitness = Nx.select(condition, gbestk_fitness_value, best_g_fitness)

    {updated_wgbestk, updated_best_fitness}
  end

  @spec find_wbest(t()) :: t()
  defp find_wbest(wso) do
    {new_w_best, new_best_fitness} =
      find_wbest(
        wso.w,
        wso.fitness_results,
        wso.best_fitness,
        wso.w_best
      )

    %{wso | w_best: new_w_best, best_fitness: new_best_fitness}
  end

  @doc """
  Expects all arguments to be valid tensors.

  Compares `fitness_results` elementwise with `best_fitness`. For each row
  where a new fitness value is lower, the corresponding row in `w` is selected
  to update `w_best`, and the new fitness is selected for `best_fitness`. Otherwise,
  the original values are retained.

  Returns a tuple: {updated_w_best, updated_best_fitness}
  """
  defn find_wbest(w, fitness_results, best_fitness, w_best) do
    # Create a boolean mask: true where fitness_results is less than best_fitness.
    mask = Nx.less(fitness_results, best_fitness)

    # Expand the mask dimensions so it can be broadcasted to the shape of w.
    mask_expanded = Nx.new_axis(mask, -1)  # Shape: {n} -> {n, 1}
    mask_broadcasted = Nx.broadcast(mask_expanded, Nx.shape(w))  # {n, 1} -> {n, d}

    # Conditionally update each row: if the mask is true, pick the new row from w,
    # otherwise keep the row from w_best.
    updated_w_best = Nx.select(mask_broadcasted, w, w_best)
    updated_best_fitness = Nx.select(mask, fitness_results, best_fitness)

    {updated_w_best, updated_best_fitness}
  end

  @spec movement_speed_towards_prey(t()) :: t()
  defp movement_speed_towards_prey(wso) do

    keys = Nx.Random.split(wso.key, parts: 4) # Generate 3 independent keys

    new_v =
      movement_speed_towards_prey_defn(wso.w,
        wso.w_best,
        wso.v,
        wso.hyperparams.mu,
        wso.p1,
        wso.p2,
        keys
      )

    %{wso | v: new_v, key: keys[3]}
  end

  defn movement_speed_towards_prey_defn(w, w_best, v, p1, p2, mu, keys) do
    {c1, _} = Nx.Random.uniform(keys[0], shape: {@n, 1})
    {c2, _} = Nx.Random.uniform(keys[1], shape: {@n, 1})
    {nu, _} = Nx.Random.randint(keys[2], 0, @n, shape: {@n}, type: :s64)
    selected_wbest = Nx.take(w_best, nu, axis: 0)
    new_v = Nx.multiply(mu, (v
          |> Nx.add(p1
            |> Nx.multiply(c1))
            |> Nx.multiply(Nx.subtract(w_best,w))
          |> Nx.add(p2
            |> Nx.multiply(c2)
            |> Nx.multiply(Nx.subtract(selected_wbest,w)))
        ))
    Nx.clip(new_v, -100, 100)
  end

  @spec movement_speed_towards_optimal_prey(t()) :: t()
  defp movement_speed_towards_optimal_prey(wso) do
    rand = wso.hyperparams.rand_fun.()
    mv = 1 / (wso.hyperparams.a0 +
      :math.exp( (wso.max_iterations/2.0 - wso.k)/ wso.hyperparams.a1 ))

    w_new = case rand < mv do
      true -> movement_speed_towards_prey_defn_yes(wso.w, wso.problem.u, wso.problem.l)
      false -> movement_speed_towards_prey_defn_no(wso.w, wso.v, wso.hyperparams.f)
    end


    %{wso | w: w_new}
    #%{wso | w: Nx.min(Nx.max(w_new, wso.problem.l), wso.problem.u)}
  end

  @doc """
  Computes the updated position when the condition (rand < mv) is true.

  This function adjusts the current solution vector `w` towards the bounds
  defined by `u` (upper) and `l` (lower). It uses tensorized operations like
  broadcasting, element-wise comparisons, and selection.
  """
  defn movement_speed_towards_prey_defn_yes(w, u, l) do
    a =
      w
      |> Nx.subtract(Nx.broadcast(u, {@n, @d}))
      |> Nx.greater(0.0)
      |> Nx.select(0.0, 1.0)

    b =
      w
      |> Nx.subtract(Nx.broadcast(l, {@n, @d}))
      |> Nx.less(0.0)
      |> Nx.select(0.0, 1.0)

    w0 = Nx.logical_and(a, b)

    Nx.add(
      Nx.add(
        Nx.multiply(w, w0),
        Nx.multiply(Nx.broadcast(u, {@n, @d}), a)
      ),
      Nx.multiply(Nx.broadcast(l, {@n, @d}), b)
    )
  end

  @doc """
  Computes the updated position when the condition (rand < mv) is false.

  This branch simply updates `w` by adding a scaled version of the velocity `v`,
  where the scaling is determined by hyperparameter `f`.
  """
  defn movement_speed_towards_prey_defn_no(w, v, f) do
    Nx.add(w, Nx.divide(v, f))
  end

  @spec update_masked_indices_towards_the_best_white_shark(t(), Nx.Tensor.t(), integer()) :: t()
  defp update_masked_indices_towards_the_best_white_shark(wso, indices, no_updates) do
      {r1_masked, new_key} = Nx.Random.uniform(wso.key, shape: {no_updates, 1})
      {r2_masked, new_key} = Nx.Random.uniform(new_key, shape: {no_updates, 1})

      w_bestk_masked = Nx.take(wso.w_best, indices, axis: 0)
      w_masked       = Nx.take(wso.w, indices, axis: 0)

      {rand_masked, new_key} = Nx.Random.uniform(new_key, shape: {no_updates, @d})

      d_masked = Nx.abs(Nx.multiply(rand_masked, Nx.subtract(w_bestk_masked, w_masked)))

      {rand, new_key} = Nx.Random.uniform(new_key, 0.0, 2.0, shape: {no_updates, @d})

      update_masked = w_bestk_masked
        |> Nx.add(Nx.multiply(Nx.multiply(r1_masked, d_masked),
                              Nx.sign(Nx.subtract(r2_masked, 0.5))))
        |> Nx.add(w_masked)
        |> Nx.divide(rand)
        |> Nx.subtract(w_masked)

      w_new = Nx.indexed_add(wso.w, Nx.new_axis(indices, -1), update_masked, axes: [0])

      %{wso | key: new_key, w: w_new}
  end

  @spec indices_where_one(Nx.Tensor.t()) :: Nx.Tensor.t()
  def indices_where_one(tensor) do
    tensor
    |> Nx.to_flat_list()
    |> Enum.with_index()
    |> Enum.filter(fn {value, _index} -> value == 1 end)
    |> Enum.map(fn {_value, index} -> index end)
    |> Nx.tensor()
  end

  @spec movement_towards_the_best_white_shark(t()) :: t()
  def movement_towards_the_best_white_shark(wso) do
    ss = abs(1 - :math.exp( -wso.hyperparams.a2 * wso.k / wso.max_iterations))

    {r3, new_key} = Nx.Random.uniform(wso.key, shape: {@n})
    mask = Nx.less(r3, Nx.tensor(ss))

    if Nx.to_number(Nx.all(Nx.logical_not(mask))) == 1 do
        wso
    else
      indices = Nx.greater(mask, Nx.tensor([0]))
        |> indices_where_one()
      no_updates = elem(Nx.shape(indices), 0)
      update_masked_indices_towards_the_best_white_shark(%{wso | key: new_key}, indices, no_updates)

    end
  end

  @spec movement_towards_the_best_white_shark_nx(__MODULE__.t()) :: __MODULE__.t()
  def movement_towards_the_best_white_shark_nx(wso) do
    # Convert scalars to tensors so they can be used in defn:
    k_tensor = Nx.tensor(wso.k)
    max_iter_tensor = Nx.tensor(wso.max_iterations)

    {w_new, new_key} =
      movement_towards_the_best_white_shark(
        wso.w,
        wso.w_best,
        wso.key,
        wso.hyperparams.a2,
        k_tensor,
        max_iter_tensor
      )

    %{wso | w: w_new, key: new_key}
  end

  defn movement_towards_the_best_white_shark(w, w_best, key, a2, k, max_iterations) do
    # Get the number of rows and columns from w.
    n = elem(Nx.shape(w), 0)
    d = elem(Nx.shape(w), 1)

    # Compute a threshold ss = |1 - exp(- (a2 * k / max_iterations))|
    ss = Nx.abs(1 - Nx.exp(-(a2 * k / max_iterations)))

    # For each row, sample a random number.
    {r3, key1} = Nx.Random.uniform(key, shape: {n})
    # Create a mask: true if the random number is less than ss.
    mask = Nx.less(r3, ss)
    # Expand mask to shape {n, d} so we can use it to select entire rows.
    mask_expanded = Nx.new_axis(mask, -1) |> Nx.broadcast({n, d})

    # Vectorized update for every row:
    {r1, key2} = Nx.Random.uniform(key1, shape: {n, 1})
    {r2, key3} = Nx.Random.uniform(key2, shape: {n, 1})
    {rand_masked, key4} = Nx.Random.uniform(key3, shape: {n, d})
    # Compute d_all = |rand_masked * (w_best - w)|
    d_all = Nx.abs(rand_masked * (w_best - w))
    {rand_val, key5} = Nx.Random.uniform(key4, 0.0, 2.0, shape: {n, d})

    # Compute the update value for every row according to the update formula.
    update_all =
      (w_best + (r1 * d_all * Nx.sign(r2 - 0.5)) + w)
      |> Nx.divide(rand_val)
      |> Nx.subtract(w)

    # Only apply the update where mask is true.
    update_to_apply = Nx.select(mask_expanded, update_all, Nx.broadcast(0.0, {n, d}))
    w_new = w + update_to_apply

    {w_new, key5}
  end

  @spec iteration(t()) :: t()
  defp iteration(wso) do
    if not wso.verbose do
      IO.write("Iteration ")
      IO.write(inspect(wso.k))
      IO.write(" curr_best: ")
      IO.write(wso.best_g_fitness |> Nx.to_number())
      IO.write(" at ")
      IO.inspect(wso.wgbestk |> Nx.to_flat_list())
    end
    movement_towards_the_best_white_shark_fun =
    if wso.fully_vectorized do
      &movement_towards_the_best_white_shark_nx/1
    else
      &movement_towards_the_best_white_shark/1
    end
    case wso.k < wso.max_iterations do
      true ->
        wso
        |> compute_ps()
        |> movement_speed_towards_prey()
        |> movement_speed_towards_optimal_prey()
        |> movement_towards_the_best_white_shark_fun.()
        |> fitness_function()
        |> find_wgbestk()
        |> find_wbest()
        |> (fn map -> Map.update!(map, :k, &(&1 + 1)) end).()
        |> iteration()
      false -> wso |> find_wgbestk() |> find_wbest()
    end
  end

  @doc """
  Executes the White Shark Optimization (WSO) algorithm on the given `WhiteSharkOptimizer` struct and returns the optimized results.

  ### Parameters:
  - `wso`: A `WhiteSharkOptimizer` struct, already initialized with the problem, hyperparameters, and optional configuration values. The struct should also have initial positions (`w`), velocities (`v`), and other necessary fields set.

  ### Returns:
  - An updated `WhiteSharkOptimizer` struct with:
    - `wgbestk`: The global best position found by the algorithm.
    - `best_g_fitness`: The fitness value of the global best solution.
    - `w_best`: The personal best positions for each individual solution in the population.
    - `best_fitness`: The fitness values corresponding to the personal best positions.

  """
  @spec run(t()) :: t()
  def run(wso) do
    wso
    |> fitness_function()
    |> find_wgbestk()
    |> find_wbest()
    |> iteration()

  end

end
