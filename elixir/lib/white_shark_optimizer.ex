defmodule WhiteSharkOptimizer do
  @moduledoc """
  Implements the White Shark Optimization algorithm for solving optimization problems in Nx.
  Based on
  https://www.sciencedirect.com/science/article/pii/S0950705122001897
  https://www.mathworks.com/matlabcentral/fileexchange/107365-white-shark-optimizer-wso
  """
  require Nx
  require Problem
  require Hyperparameters
  
  @type t :: %WhiteSharkOptimizer{
          problem: Problem.t() | nil,
          hyperparams: Hyperparameters.t | nil,
          key: integer() | nil,
          w: Nx.Tensor.t | nil,
          v: Nx.Tensor.t | nil,
          k: integer(),
          max_iterations: integer(), #called K in the paper
          p1: float() | nil,
          p2: float() | nil,
          wgbestk: Nx.Tensor.t() | nil,
          best_g_fitness: float() | nil,
          w_best: Nx.Tensor.t() | nil,
          best_fitness: Nx.Tensor.t | nil,
          fitness_results: Nx.Tensor.t | nil,
          verbose: boolean(), 
        }
  
  defstruct problem: nil,
            hyperparams: nil,
            key: nil,
            w: nil,
            v: nil,
            k: 0,
            max_iterations: 100,
            p1: nil,
            p2: nil,
            wgbestk: nil,
            best_g_fitness: :infinity,
            w_best: nil,
            best_fitness: nil,
            fitness_results: nil,
            verbose: true

  
  @spec compute_ps(t()) :: t()
  defp compute_ps(wso) do
    p_min = wso.hyperparams.p_min
    p_max = wso.hyperparams.p_max
    p1 = p_max + (p_max - p_min) * :math.exp(-:math.pow( 4 * wso.k / wso.max_iterations, 2))
    p2 = p_min + (p_max - p_min) * :math.exp(-:math.pow( 4 * wso.k / wso.max_iterations, 2))
    %{wso | p1: p1, p2: p2}
  end
  
  @doc """
  Initializes a new instance of the WhiteSharkOptimizer struct with the provided problem, hyperparameters, and optional configuration.
  
  ### Parameters:
  - `problem`: A struct defining the optimization problem to be solved. It must include the following fields:
    - `d`: Dimensionality of the problem.
    - `l`: Lower bounds for the search space as an Nx.Tensor of shape `{d}`.
    - `u`: Upper bounds for the search space as an Nx.Tensor of shape `{d}`.
    - `fun`: A fitness function of the form `(Nx.Tensor.t() -> float())`, used to evaluate the quality of solutions.
    - The dimensionality (`d`) must match the bounds (`l` and `u`).
  - `hyperparams`: A struct specifying the algorithm's hyperparameters such as the population size (`n`), learning rate (`mu`), and others necessary for controlling the behavior of the optimizer.
  - `opts`: A map of optional configuration values. These can include:
    - `key`: Random key for generating initial population and randomness. Defaults to a key generated by `Nx.Random.key(0)` if not provided.
    - `verbose`: If `false`, the best solution will be printed every iteration. Defaults to `true`.
  
  ### Raises:
  - `ArgumentError`: Raised in the following cases:
    - `problem` is `nil`, as the optimizer cannot operate without a defined problem.
    - `problem` is missing required fields (`d`, `l`, `u`, or `fun`).
    - The dimensions of the bounds (`l` and `u`) do not match the dimensionality (`d`).
    - The `fun` field is not a valid function.
  
  ### Returns:
  - A `WhiteSharkOptimizer` struct initialized with computed fields.
  """
  @spec new(Problem.t(), Hyperparameters.t(), map()) :: t()
  def new(%{d: _, l: _, u: _, fun: _} = problem, hyperparams, opts \\ %{}) do
    validate_problem(problem)

    key = Map.get(opts, :key, Nx.Random.key(0))

    {random_tensor, key} = Nx.Random.uniform(key, shape: {hyperparams.n, problem.d})
    w_initial = random_tensor 
          |> Nx.multiply(Nx.subtract(problem.u, problem.l))
          |> Nx.add(problem.l)
    computed_fields = %{
      w: w_initial,
      v: Nx.broadcast(0, {hyperparams.n, problem.d}),
      key: key,
      problem: problem,
      hyperparams: hyperparams,
      w_best: w_initial,
      best_fitness: Nx.broadcast(Nx.Constants.infinity(), {hyperparams.n})
    }
    
    %__MODULE__{}
    |> struct(Map.merge(computed_fields, opts))
    |> compute_ps()
    
  end
    
  defp validate_problem(%{d: d, l: l, u: u, fun: fun}) do
    unless is_integer(d) and d > 0 do
      raise ArgumentError, "`d` must be a positive integer"
    end
  
    unless is_function(fun, 1) do
      raise ArgumentError, "`fun` must be a valid function of the form `Nx.Tensor.t() -> float()`"
    end
  
    unless Nx.axis_size(l, 0) == d and Nx.axis_size(u, 0) == d do
      raise ArgumentError, "The dimensions of `l` and `u` must match `d` in the problem struct"
    end
  end

  defp validate_problem(_) do
    raise ArgumentError, "Problem struct must include fields `d`, `l`, `u`, and `fun`"
  end
      
  @spec fitness_function(t()) :: t()
  defp fitness_function(wso) do
  
    # Process each row and compute the fitness results
    fitness_results = 
      Enum.map(0..(wso.hyperparams.n - 1), fn i ->
        wso.w
        |> Nx.slice([i, 0], [1, Nx.axis_size(wso.w, 1)])
        |> Nx.squeeze()
        |> wso.problem.fun.()
      end)
    |> Nx.stack()
  
    # Update the struct with computed fitness results
    %{wso | fitness_results: fitness_results}
  end

  @spec find_wgbestk(t()) :: t()
  defp find_wgbestk(wso) do
    gbestk = Nx.argmin(wso.fitness_results)
    gbestk_fitness_value = wso.fitness_results
      |> Nx.slice([gbestk], [1]) 
      |> Nx.reshape({})
      |> Nx.to_number()
    if gbestk_fitness_value < wso.best_g_fitness do
      %{wso | wgbestk: Nx.slice(wso.w, [gbestk, 0], [1, Nx.axis_size(wso.w, 1)]), 
      best_g_fitness: gbestk_fitness_value}
    else
      wso
    end
  end
  
  @spec find_wbest(t()) :: t()
  defp find_wbest(wso) do
    # Create a mask for rows where fitness_results < best_fitness
    mask = Nx.less(wso.fitness_results, wso.best_fitness)
  
    # Expand the mask to align dimensions (add an axis to match {n, d})
    mask_expanded = Nx.new_axis(mask, -1)  # Shape: {n} -> {n, 1}
  
    # Broadcast the mask to match the shape of w and w_best
    mask_broadcasted = Nx.broadcast(mask_expanded, Nx.shape(wso.w))  # Shape: {n, 1} -> {n, d}
  
    # Perform conditional updates with Nx.select
    updated_w_best = Nx.select(mask_broadcasted, wso.w, wso.w_best)
    updated_best_fitness = Nx.select(mask, wso.fitness_results, wso.best_fitness)
  
    # Return the updated struct
    %{wso |
      w_best: updated_w_best,
      best_fitness: updated_best_fitness}
  end
    
  @spec movement_speed_towards_prey(t()) :: t()
  defp movement_speed_towards_prey(wso) do 
    
     {c1, new_key} = Nx.Random.uniform(wso.key, shape: {wso.hyperparams.n, 1})
     {c2, new_key} = Nx.Random.uniform(new_key, shape: {wso.hyperparams.n, 1})

     {rand, new_key} = Nx.Random.uniform(new_key, 0.0, wso.hyperparams.n, shape: {wso.hyperparams.n}) 
  
     nu = Nx.floor(rand) |> Nx.as_type(:s64) 
     selected_wbest = Nx.take(wso.w_best, nu, axis: 0)

     new_v = Nx.multiply(wso.hyperparams.mu, (wso.v
          |> Nx.add(wso.p1 
            |> Nx.multiply(c1)
            |> Nx.multiply(Nx.subtract(wso.w_best, wso.w)))
          |> Nx.add(wso.p2
            |> Nx.multiply(c2)
            |> Nx.multiply(Nx.subtract(selected_wbest, wso.w)))
        ))

     %{wso | 
      v: new_v,
      key: new_key}

    #rmin = 1.0
    #rmax = 3.0
    # {rand, new_key} = Nx.Random.uniform(wso.key, 0.0, wso.hyperparams.n, shape: {wso.hyperparams.n}) 
    #nu = Nx.floor(rand) |> Nx.as_type(:s64) 
    
    # {rr, new_key} = Nx.Random.uniform(new_key, rmin, rmax, shape: {wso.hyperparams.n})
    #wr=abs(((2*rand()) - (1*rand()+rand()))/rr);
    # {r1, new_key} = Nx.Random.uniform(new_key, 0.0, 2.0, shape: {wso.hyperparams.n}) 
    # {r2, new_key} = Nx.Random.uniform(new_key, shape: {wso.hyperparams.n}) 
    # {r3, new_key} = Nx.Random.uniform(new_key, shape: {wso.hyperparams.n}) 

    #selected_wbest = Nx.take(wso.w_best, nu, axis: 0)
    
    #wr = r1
    #  |> Nx.subtract(Nx.add(r2, r3))
    #  |> Nx.divide(rr)
    #  |> Nx.abs()
      

    #new_v = wso.hyperparams.mu
    #  |> Nx.multiply(wso.v)
    #  |> Nx.add(
    #    wr
    #    |> Nx.broadcast({wso.hyperparams.n, wso.problem.d}, axes: [0])
    #    |> Nx.multiply(Nx.subtract(selected_wbest, wso.w))
    #  )
    #%{wso | 
    #      v: new_v,
    #      key: new_key}
  end

  @spec movement_speed_towards_optimal_prey(t()) :: t()
  defp movement_speed_towards_optimal_prey(wso) do
    rand = wso.hyperparams.rand_fun.()
    mv = 1 / (wso.hyperparams.a0 + 
      :math.exp( (wso.max_iterations/2.0 - wso.k)/ wso.hyperparams.a1 ))

      w_new = case rand < mv do
      true -> 
        a = wso.w
          |> Nx.subtract(Nx.broadcast(wso.problem.u, {wso.hyperparams.n, wso.problem.d}))
          |> Nx.greater(0)
          |> Nx.select(0, 1)

        b = wso.w
          |> Nx.subtract(Nx.broadcast(wso.problem.l, {wso.hyperparams.n, wso.problem.d}))
          |> Nx.less(0)
          |> Nx.select(0, 1)

        w0 = Nx.logical_and(a, b) 
        # NOT XOR (a, b) = AND (NOT a, NOT a) note that a and b cannot both be 1
        
        wso.w 
           |> Nx.multiply(w0)
           |> Nx.add(Nx.multiply(wso.problem.u, a))
           |> Nx.add(Nx.multiply(wso.problem.l, b))

      false -> wso.w |> Nx.add(Nx.divide(wso.v, wso.hyperparams.f))
    end
    %{wso | w: w_new}
    #%{wso | w: Nx.min(Nx.max(w_new, wso.problem.l), wso.problem.u)}
  end

  
  @spec update_masked_indices_towards_the_best_white_shark(t(), Nx.Tensor.t(), integer()) :: t()
  defp update_masked_indices_towards_the_best_white_shark(wso, indices, no_updates) do
      {r1_masked, new_key} = Nx.Random.uniform(wso.key, shape: {no_updates, 1})
      {r2_masked, new_key} = Nx.Random.uniform(new_key, shape: {no_updates, 1})

      w_bestk_masked = Nx.take(wso.w_best, indices, axis: 0)
      w_masked       = Nx.take(wso.w, indices, axis: 0)

      {rand_masked, new_key} = Nx.Random.uniform(new_key, shape: {no_updates, wso.problem.d})

      d_masked = Nx.abs(Nx.multiply(rand_masked, Nx.subtract(w_bestk_masked, w_masked)))

      {rand, new_key} = Nx.Random.uniform(new_key, 0.0, 2.0, shape: {no_updates, wso.problem.d})

      update_masked = w_bestk_masked
        |> Nx.add(Nx.multiply(Nx.multiply(r1_masked, d_masked),
                              Nx.sign(Nx.subtract(r2_masked, 0.5))))
        |> Nx.add(w_masked)
        |> Nx.divide(rand)
        |> Nx.subtract(w_masked)

      w_new = Nx.indexed_add(wso.w, Nx.new_axis(indices, -1), update_masked, axes: [0])

      %{wso | key: new_key, w: w_new}
  end

  @spec indices_where_one(Nx.Tensor.t()) :: Nx.Tensor.t()
  def indices_where_one(tensor) do
    tensor
    |> Nx.to_flat_list()
    |> Enum.with_index()
    |> Enum.filter(fn {value, _index} -> value == 1 end)
    |> Enum.map(fn {_value, index} -> index end)
    |> Nx.tensor()
  end
  
  @spec movement_towards_the_best_white_shark(t()) :: t()
  defp movement_towards_the_best_white_shark(wso) do
    ss = abs(1 - :math.exp( -wso.hyperparams.a2 * wso.k / wso.max_iterations))

    {r3, new_key} = Nx.Random.uniform(wso.key, shape: {wso.hyperparams.n})
    mask = Nx.less(r3, Nx.tensor(ss))
    
    if Nx.to_number(Nx.all(Nx.logical_not(mask))) == 1 do
        wso
    else
      indices = Nx.greater(mask, Nx.tensor([0]))
        |> indices_where_one()
      no_updates = elem(Nx.shape(indices), 0)
      update_masked_indices_towards_the_best_white_shark(%{wso | key: new_key}, indices, no_updates)
      
    end
  end
    
  @spec iteration(t()) :: t()
  defp iteration(wso) do
    if not wso.verbose do
      IO.write("Iteration ")
      IO.write(inspect(wso.k))
      IO.write(" curr_best: ")
      IO.write(wso.best_g_fitness)
      IO.write(" at ")
      IO.inspect(wso.wgbestk |> Nx.to_flat_list())
    end
    
    case wso.k < wso.max_iterations do
      true -> 
        wso
        |> compute_ps()
        |> movement_speed_towards_prey()
        |> movement_speed_towards_optimal_prey()
        |> movement_towards_the_best_white_shark()
        |> fitness_function()
        |> find_wgbestk()
        |> find_wbest()
        |> (fn map -> Map.update!(map, :k, &(&1 + 1)) end).()
        |> iteration()
      false -> wso |> find_wgbestk() |> find_wbest()
    end
  end
  
  @doc """
  Executes the White Shark Optimization (WSO) algorithm on the given `WhiteSharkOptimizer` struct and returns the optimized results.
  
  ### Parameters:
  - `wso`: A `WhiteSharkOptimizer` struct, already initialized with the problem, hyperparameters, and optional configuration values. The struct should also have initial positions (`w`), velocities (`v`), and other necessary fields set.
  
  ### Returns:
  - An updated `WhiteSharkOptimizer` struct with:
    - `wgbestk`: The global best position found by the algorithm.
    - `best_g_fitness`: The fitness value of the global best solution.
    - `w_best`: The personal best positions for each individual solution in the population.
    - `best_fitness`: The fitness values corresponding to the personal best positions.
  
  """
  @spec run(t()) :: t()
  def run(wso) do
    wso   
    |> fitness_function()
    |> find_wgbestk()
    |> find_wbest()
    |> iteration()
    
  end

end